import os

import nni
import numpy as np
import pandas as pd
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score, recall_score

from pylibs.common import ConstMetric
from pylibs.evaluation.contextual import contextual_confusion_matrix
from pylibs.evaluation.utils import from_list_points_labels
from pylibs.utils.util_log import get_logger
from pylibs.utils.utils import get_random_file_name

log = get_logger()


class MetricHelper:
    """
    The threshold is generated by np.linespace(0.01,1,100).

    """

    def __init__(self):

        # 当前数据的所有，集第 k 次交叉验证
        self.alert_delay = {}
        self.best_threshold = {}
        self.curr_kfold_index = 0

        self.predict_score = {}
        self.stack_predict_mscore = {}
        self.predict_values = {}
        self.test_labels = {}
        self.test_values = {}

        self.stack_k_fold_aucs = {}
        self.best_f1_score = {}
        self.best_precision = {}
        self.best_recall = {}

    def report_final_result(self, test_x, test_label, test_predict_value, test_predict_score):
        """
        更新交叉验证的数据


        Parameters
        ----------
        test_x : np.ndarray
            The test data set
        test_label : list
            The value
        test_predict_value : np.ndarray
            Predict x

        test_predict_score : np.ndarray
            Score to predict, which is negative log probability,
            The small test_predict_score, the less likely a point
            is anomaly.  You may take the negative of the score, if you want
            something to directly indicate the severity of anomaly.

        Returns
        -------
        list,array
            A set of metrics
        list
            bbb
        """
        # Check parameters
        assert len(test_x) == len(test_label)
        assert len(test_x) == len(test_predict_value)
        assert len(test_x) == len(test_predict_score)

        # Ensure that labels exist at least an anomaly
        assert np.sum(test_label) > 0

        self.test_values = test_x
        self.test_labels = test_label
        self.predict_values = test_predict_value
        self.predict_score = test_predict_score

        # f1, precision, recall, threshold
        ground_truth_window = from_list_points_labels(test_label)
        metrics = self.get_curr_metrics_with_modified_score(
            ground_truth_window,
            self.predict_score
        )

        _best_f1_score_index = int(np.argmax(metrics[:, 0]))
        self.best_precision = metrics[:, 1][_best_f1_score_index]
        self.best_recall = metrics[:, 2][_best_f1_score_index]
        self.best_f1_score = metrics[:, 0][_best_f1_score_index]
        self.best_threshold = metrics[:, 3][_best_f1_score_index]

        plot_timeseries(test_x, ground_truth=ground_truth_window, detected_window=metrics[:, 4][_best_f1_score_index],
                        score=test_predict_score, threshold=self.best_threshold)

        _predict_label = test_predict_score >= self.best_threshold
        self.alert_delay = self.get_alert_delay(test_label, _predict_label)

        self._report_final_metric()

    def _report_final_metric(self, save_to_file=False):
        final_metric = {
            ConstMetric.KEY_DEFAULT: self.best_f1_score,
            ConstMetric.KEY_BEST_PRECISION: self.best_precision,
            ConstMetric.KEY_BEST_RECALL: self.best_recall,
            ConstMetric.KEY_ALERT_DELAY: self.alert_delay,
            ConstMetric.KEY_BEST_THRESHOLD: self.best_threshold
        }

        # Report final result for nni
        nni.report_final_result(final_metric)
        log.info(f"\n=========== Final metric ========="
                 f"\n{final_metric}")

        if save_to_file:
            rf = get_random_file_name(ext=".csv")
            pd.DataFrame(final_metric).to_csv(rf)
            log.info(f"Save final metric to {os.path.abspath(rf)}")
        return final_metric

    def modified_label(self, y_true, predict_label):
        """
        实现的逻辑不对。现在是根据真实 label 来做的，但实际上是要根据auc来算
        Modified metrics;that is, identify the anomaly by segment, not a point.
        If any point in an anomaly segment in the ground truth can be detected by a chosen threshold,
        we say this segment is detected correctly, and all points in this segment are treated as if they can be detected
        by this threshold.
        Examples
        ----------
        y_true =    [1,      1,      1,       0, 0, 0,   1,    1,      1,      1,     0,    1]
        score  =    [-0.1,  -0.3,   -0.1,     0, 0, 0,   0,    0,      0,     -0.5,   0,    0]

        modified =  [-0.3,  -0.3,   -0.3,     0, 0, 0,  -0.5,  -0.5,  -0.5,   -0.5,   0,    0]
        Parameters
        ----------
        predict_label: 1-D np.array
        y_true: 1-D np.array

        Returns
        -------
        1-D np.array

        """

        y_true = np.asarray(y_true, dtype=int)
        predict_label = np.asarray(predict_label, dtype=int)

        assert y_true.shape[0] == predict_label.shape[0]
        assert len(y_true.shape) == 1
        assert len(predict_label.shape) == 1
        modified_label = predict_label.copy()
        anomaly_span_index_of_truth = self.get_anomalies_span_index(y_true)
        for ts in anomaly_span_index_of_truth:
            # Set to anomaly in the widget.
            predict_anomaly_window = modified_label[ts]
            if np.sum(predict_anomaly_window) > 0:
                modified_label[ts] = np.repeat(1, len(ts))
        return modified_label

    def get_anomalies_span_index(self, y_true):
        """
        获取数据的异常区间的索引
        Parameters
        ----------
        y_true

        Returns
        -------

        """
        anomaly_span_index_of_truth = []
        size_of_y = len(y_true)
        _s = 0
        _n = 0
        for i in range(len(y_true)):
            if y_true[i] == 1:
                _n = i + 1
                if i == size_of_y - 1:
                    anomaly_span_index_of_truth.append(np.arange(start=_s, stop=_n, step=1))
            else:
                if _n > _s:
                    anomaly_span_index_of_truth.append(np.arange(start=_s, stop=_n, step=1))
                _s = i + 1
        return anomaly_span_index_of_truth

    def get_alert_delay(self, y_true: list, predict_label: np.ndarray):
        """
        计算 y_true 和 predict_label 的 alert delay
        Parameters
        ----------
        y_true: list
            (n_samples,), 二分类
        predict_label: np.ndarray
            (n_samples,）, 二分类

        Returns
        -------

        """
        assert type(predict_label) == np.ndarray
        assert type(y_true) == np.ndarray

        alert_delay = 0
        spans = self.get_anomalies_span_index(y_true)
        for span_index in spans:

            _predict = predict_label[span_index]

            # 至少有一个1，否者不存在误报
            if np.sum(_predict) == 0:
                continue

            # alert 最起码需要两个点
            if _predict.shape[0] < 2:
                continue
            # 第一个点为0 才存在alert delay
            if _predict[0] == 1:
                continue

            for index, _p in enumerate(_predict):
                if _p == 0:
                    alert_delay = alert_delay + 1
                if _p > 0:
                    # 是 1， 表示已经开始预测了
                    break
        return alert_delay

    def _score_normalize(self, predict_score):
        _min = np.min(predict_score)
        _max = np.max(predict_score)
        return (predict_score - _min) / (_max - _min)

    def get_curr_metrics_with_modified_score(self, true_label, predict_score):
        """
        Get metrics with modified score, seeing https://dl.acm.org/doi/abs/10.1145/3178876.3185996

        return: auc,thresholds,precision,recalls,f1_scores, tpr, fpr

        Parameters
        ----------
        true_label : list
        predict_score : np.ndarray
            The log probability of the predict x.

        is_modified_metric: bool
            Do not change the parameter, this is using in test function

        Returns
        -------
        auc:float
            The Area Under Curve

        thresholds: np.ndarray
            The threshold with associated precision and recall

        precisions:np.ndarray
            The precision corresponding to the threshold

        recalls: np.ndarray
            The precision corresponding to the threshold

        f1s: np.ndarray

            The f1 scores corresponding to the threshold
        tpr: np.ndarray
        fpr: np.ndarray
        """

        _metrics = []
        # Notice: The  f1, precision, and recall are equal to 1 if  the threshold = 0.
        for threshold in np.linspace(np.percentile(predict_score, 20), np.max(predict_score), 100):
            predict_label = np.asarray(np.where(predict_score >= threshold, 1, 0), dtype=int)
            predict_window = np.asarray(from_list_points_labels(predict_label))
            if not self.is_invalid_preidict_window(predict_window):
                continue

            f1, precision, recall = self.calculate_f1_pre_rec(true_label, predict_window)
            UtilSys.is_debug_mode() and log.info(f"Threshold:{threshold},Prec: {precision},recall:{recall},f1: {f1}")

            _metrics.append([f1, precision, recall, threshold, predict_window])

        # >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)
        # >>> metrics.auc(fpr, tpr)
        _metrics = np.asarray(_metrics)

        return _metrics

    def calculate_f1_pre_rec(self, ground_truth, predict_window):
        """
        f1 , precision, recall, tpr, fpr

        Parameters
        ----------
        predict_window :
        ground_truth :

        Returns
        -------
        tuple
             f1, precision, recall, tpr, fpr

        """
        if len(predict_window) == 0:
            return np.zeros((3,))
        # number of true negative, false positive, false negative, true positive.

        # log.info(f"Ground truth: {ground_truth},Predict:{predict_window}")
        ground_truth = list(ground_truth)
        predict_window = list(predict_window)
        tn, fp, fn, tp = contextual_confusion_matrix(ground_truth, predict_window, weighted=False)
        precision = 0. if tp + fp == 0 else np.divide(tp, tp + fp)
        recall = 0. if tp + fn == 0 else np.divide(tp, (tp + fn))
        f1 = 0.0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)
        UtilSys.is_debug_mode() and log.info(f"Tn:{tn},fp:{fp},fn:{fn},tp:{tp}")
        return f1, precision, recall

    def report_test_metric(self, true_label, predict_scores, threshold):
        predict_label = predict_scores >= threshold
        _prec = precision_score(true_label, predict_label)
        _recall = recall_score(true_label, predict_label)
        _f1_score = f1_score(true_label, predict_label)
        _alert_delay = self.get_alert_delay(true_label, np.asarray(predict_label))
        UtilSys.is_debug_mode() and log.info(f"\nModel final metric on threshold = [{threshold}] on test set: "
                                             f"\nprecision: {_prec},recall:{_recall},f1_score:{_f1_score},alert_delay:{_alert_delay}")

    def normal_score(self, predict_scores):
        _min = np.min(predict_scores)
        _max = np.max(predict_scores)
        return (predict_scores - _min) / (_max - _min)

    def is_invalid_preidict_window(self, predict_window):
        _max_windows_detected = np.max(predict_window[:, -1] - predict_window[:, 0])
        # todo the detected windows are error
        n_label = len(self.test_labels)

        if _max_windows_detected > n_label / 2:
            return False
        else:
            return True
