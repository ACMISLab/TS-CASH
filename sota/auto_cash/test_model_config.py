conf = {'activation': 'relu', 'alpha': 0.087049, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999,
        'early_stopping': 'train', 'epsilon': 0.0, 'hidden_layer_depth': 3, 'learning_rate_init': 0.002487,
        'n_iter_no_change': 32, 'num_nodes_per_layer': 262, 'random_state': 42, 'shuffle': 'True', 'solver': 'adam',
        'tol': 0.0001, 'validation_fraction': 0.1}
